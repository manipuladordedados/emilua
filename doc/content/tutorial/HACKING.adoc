+++
title = "HACKING"
+++

:toc: macro
:_:
:cpp: C++

WARNING: The target public for this document are {cpp} programmers who want to
delve into the project's code, not lua users. Native plug-in authors should also
read this page.

The intent of this page is not to detail every internal of the project, but just
to give an overview of the architecture. Details change quickly and
documentation would lag behind, so they're avoided.

Once you read it, you should be familiar with the assumptions made thoroughly
the project, and how to interact with the native code.

We assume that you already have some familiarity with the lua C API and
Boost.Asio.

toc::[]

== Multiple lua VMs

The project allows multiple OS threads to call `asio::io_context::run()`, so lua
VMs can jump from one thread to another freely, but they will always refer to
the same `asio::io_context` and each will be protected by its own ASIO strand.

[source,lua]
----
-- Instantiates a new lua VM that shares the caller's `asio::io_context`
spawn_vm(module)

-- Instantiates a new lua VM in a new thread with its own `asio::io_context`
spawn_vm(module, { inherit_ctx=false })
----

You must specify a lua module name to run in the new VM, not a function. The
module will be loaded and run in the new VM.

The only way for two different lua VMs to communicate is message passing. The
channels are given when you instantiate the extra VMs. The channels accept a
range of different values and will deep-copy them. You can also send references
to IO objects, but the original references will be rendered unusable (their
metatables are unset). Do pay attention to not let objects that have pending
operations to be sent over (`EBUSY`, but do create an error code just for that).

Nor synchronization primitives (such as `sync.mutex`) nor fiber handles can be
sent over the channels and by implication can't be used to synchronize (or send
cancellation requests to) fibers running in different lua VMs.

You can also send a channel over a channel. This will only send the channel
“address” over and will allow complex routing among the lua VMs. If you send a
channel's rx-end, the other side will receive a tx-channel anyway. On the
{cpp}-side, we need to implement a MPSC strand-based channel.

These characteristics should be enough to implement actor patterns. And it is
not the job of emilua to enforce good patterns on applications. The patterns can
be configured purely in the lua side of coding.

[source,lua]
----
-- Spawn extra threads to the caller's `asio::io_context`
spawn_ctx_threads(count)
----

Leaving the actor model aside for a moment, it's now easy to have threads with
work-stealing (e.g. 8 lua VMs sharing the same `asio::io_context` running on 4
threads) so you don't have to worry about load-balancing.

== Inside a single lua VM

When you issue some IO operation (including `chan:recv()`), the calling fiber
will suspend, but other fibers from the same lua VM are allowed to kick in
(cooperative multitasking). Fibers can share state with each other safely (and
free from contention problems) as-if the program was single-threaded.

[source,lua]
----
-- Spawn a new fiber on this lua VM
spawn(fn)
----

You can use the fiber handle just like you'd use a thread handle. There is
`join()`, `detach()` and `interrupt()`. Thanks to `interrupt()`, every async
operation on the {cpp} side must be cancellable (including `chan:recv()`).

All sync primitives obey some characteristics thanks to the restrictions we've
laid out:

* They always live in the same strand. They never migrate strands.
* They don't synchronize with fibers from other strands (except for channels,
  but that's another story).

Given these conditions, it's now easier to implement and reason about the {cpp}
code.

== Userdata practices

Besides the common practices to create custom objects through userdata, Emilua
(IO) objects will also:

* Hide the metatable. By doing that, user code is prevented from changing the
  metatable (the metatable is just an usual table after all) that native code
  relies on.

== {cpp} async operations

Let's begin with `require()`.

``require()``'ing a module is also an async operation which will suspend the
caller fiber. Every module has its own isolated environment (i.e. a new lua
thread is created for every module and that thread's environment is configured
to use a separate lua table) sharing the same lua VM. The module's entry point
is an user-provided source code evaluated to prepare the environment with the
names that should be exported to the caller fiber. But this preparatory step may
not be immediately ready and may need to call other async operations. The rule
we define to mark a module as loaded and ready is when its main fiber finishes
(synchronization code similar to `fiber:join()`).

To further enforce a more manageable project layout, it is only allowed to
import new modules from the main fiber. This may introduce a “slow” startup in
some project layouts, but:

* It is simpler to reason about the relationship of exported/imported names if
  we restrict them to the same main fiber. One such use we do of this feature is
  detecting whether the `inbox` module was loaded and close it if not.
* We are explicitly not aiming for remote modules (e.g. JS running on a web
  browser), so we don't need to care about slow startup happening in this event.
* In the cases where some module startup is indeed slow, the module programmer
  himself can adopt lazy loading techniques within his module's functions to
  have a quick startup with respect to the rest of the application.

Modules evaluate only once and are cached. We never unload them. We keep a
reference to their lua thread for as long as the lua VM is active.

Loading a module forms a loader-loaded relationship. This relationship builds a
chain that must be checked when a new module is ``require()``d (so we can for
instance prevent cyclic imports). But each module will have its own
environment. This means the {cpp} function that implements `require()` needs to
check lua-hidden state associated with the caller lua function (not a global
one). That's the module system state per-module.

[NOTE]
--
[discrete]
== Rule

The per-module state is stored by using the module's main thread as a key in the
fibers table. The fibers table is strong, but this isn't a problem because the
module shall never be unloaded anyway. Code that unrefs fiber coroutines shall
check whether the lua thread represents a module and skip removing it from the
fibers table if so.
--

We can't store the module system data directly at the thread environment because
lua code can change the thread environment by calling `setfenv(0, table)`.

We've already gone through the trickiest parts and added the most important
restrictions to the table (no lua-related pun intended), so the remaining rules
should be quick'n'easy to catch.

When you initiate an async operation, the {cpp} side will copy the `lua_State*`
to handle the completion (or cancellation) later. However, any `LUA_ERRMEM` will
trigger an emilua-call to `lua_close()` and `L` may then be invalid when we
later try to resume it. So the completion handler need to check whether the vm
is still valid before accessing it and this is the purpose of the `vm_context`
structure (also protected by the same strand as the vm).

== `this_fiber`

As long as lua code is executing, there is a current fiber and this property
stays unchanged for as long as control doesn't go back to {cpp} code.

[quote]
....
(definitions)
%
transparent, adj.:
        Being or pertaining to an existing, nontangible object.
        "It's there, but you can't see it"
                -- IBM System/360 announcement, 1964.

virtual, adj.:
        Being or pertaining to a tangible, nonexistent object.
        "I can see it, but it's not there."
                -- Lady Macbeth.
....

This property is mostly transparent to lua code. Which is to say that the
programmer is aware of this property, but there isn't a tangible object that it
can track back to `this_fiber`. This is *mostly* true, but there is a quite
tangible `this_fiber` lua global object that the user can inspect — exposed at
the beginning of the first thread execution.

However, `this_fiber` being a global is shared among all the fibers, so it can't
point to a single fiber. Instead, it will query which fiber is current and do
operations on it.

{cpp} async ops will always store which fiber is current to know how to resume
it back. And before a fiber is resumed, this info is stored at a know lua
registry's index so future async ops will get to know about it too. The reason
why we can't rely on the `L` argument passed to C functions registered at the VM
and the current fiber needs to be remembered is because there will be a `L` that
points to the wrong lua thread as soon as the user wraps some function in a
coroutine.

This design works well because we don't mix responsibilities of the scheduler
with user code (as is the case for `Fiber#resume` in Ruby which would be better
suited by a `Fiber#spawn()` that accepts ``post``/``dispatch`` execution
policies and would avoid the (un-)parking unsound ideas altogether).

== `LUA_ERRMEM`

Lua code cannot recover from allocation failures. As an example (and single-VM
only):

[source,lua]
----
my_mutex:lock()
scope_cleanup_push(function() my_mutex:unlock() end)
----

If the VM fails to allocate the closure passed to `scope_cleanup_push()`,
`my_mutex` will be kept locked and the lua code inside that VM will be in an
unrecoverable state. There's no pattern or ordering to make resource management
work here as allocation failures can happen almost anywhere and we then inherit
some constraints and reasoning from preemptive scheduling. The only option (and
this applies to *any* allocation failure reported by the lua VM) is to terminate
the VM from the {cpp}-side.

When `lua_close()` is called, there is no guarantee pending operations will be
canceled as they might hold strong references to the underlying IO object
preventing its destructor from getting called. Therefore, the `vm_context`
structure also holds an intrusive container of polymorphic elements which are
destroyed after `lua_close()` is called and can be used to register clenaup code
to avoid such leaks. If the operation finishes, the IO object is free to reclaim
their own objects from this container and use them for other purposes.

`lua_CFunction` objects should never call `lua_close()`. If they detect
`LUA_ERRMEM` all they have to do is to mark the flags field from `vm_context`
and suspend the fiber. The runtime will take care of closing `lua_State*` and
extra cleanup when it recovers control of the thread.

== Channels and resources

The biggest challenge to cross-VM resource management are the multi-strand sync
primitives (i.e. the channels). They have to execute code that jumps from one
strand to another to finish their jobs. If the associated execution context
already finished, then they would be stuck forever. The solution is for them to
keep the execution context busy through a work guard.

However some rules are needed to make this work:

* Rx-channels (i.e. `inbox`) don't keep work guards.
* Tx-channels keep a work guard to the other end while they are alive. But they
  only keep a work guard to their own strands when they have an active
  operation.

If the tx-channels are not closed, they will prevent execution contexts that are
no longer necessary from being destroyed. But that's the best we can do. We
could periodically call the GC to free unused channels, but so could lua code
and there's nothing left for us to do on the {cpp} side. A good practice for lua
code would be to add the following chunk at the beginning of the fiber who's
gonna process the actor messages:

[source,lua]
----
scope_cleanup_push(function() inbox:close() end)
----

Extra rules for channels management:

* As an extra safety measure, if the main fiber finishes and `inbox` wasn't
  imported, the runtime closes it.
* Channels (tx and rx) also get closed when the VM is terminated.

== The exception mechanism

{cpp} exceptions must not be used to propagate errors across lua/{cpp}
frames. However, lua errors may simply trigger stack unwinding (the code makes
heavy use of `setjmp()`) and we do depend on RAII to keep the code correct.

It is assumed that any call to `lua_error()` will behave as-if it throws a {cpp}
exception (thus triggering our destructors). We require some support from the
luaJIT VM for this. Specifically, we can't rely on
http://luajit.org/extensions.html#exceptions[the “no interoperability” category
from their “exception” section on the “extensions” page] because the following
restriction:

[quote]
____
Throwing Lua errors across {cpp} frames will not call {cpp} destructors.
____

To make matters worse, the feature we do depend on only appears in the the “full
interoperability” category:

[quote]
____
Throwing Lua errors across {cpp} frames is safe. {cpp} destructors will be
called.
____

A different approach would be to implement an exception mechanism in terms of
coroutines (although it'd add to code complexity):

[quote, leafo, 'http://leafo.net/posts/itchio-and-coroutines.html#overview-of-coroutines[leafo.net]']
____
----
Exceptions < Coroutines < Continuations
----

Exceptions can be thought of as a subclass of coroutines. You can implement an
exception mechanism with coroutines.
____

But this path would be a dead-end as native lua errors would still be reported
through `lua_error()`. For luaJIT, `lua_error()` plays well with our code
because:

[quote, 'http://luajit.org/extensions.html#resumable']
____
The LuaJIT VM is fully resumable. This means you can yield from a coroutine even
across contexts, where this would not possible with the standard Lua 5.1 VM:
e.g. you can yield across `pcall()` and `xpcall()`, across iterators and across
metamethods.
____

Wasn't for this guarantee, the project would be monstrous. To understand why
this guarantee is important, let's unravel the fundamental pattern for fibers
support. We always implicitly wrap every user code inside a lua coroutine:

[source,lua]
----
local fib = coroutine.create(user_fn)
----

So async operations can suspend the calling fiber and resume them later.

But the `user_fn` might very well contain a `pcall()` and execute our suspending
async function inside it:

[source,lua]
----
function user_fn()
    pcall(function()
        io_obj:emilua_async_op()
    end)
end
----

The exception mechanism should not block our ability to suspend fibers. When our
own native code calls `lua_yield()` to suspend a fiber, the suspension mechanism
should be able to cross the `pcall()` barrier.

To wrap all up so far, the standard lua exception mechanism is used to report
errors. The only difference is that emilua will `lua_error()` a structured error
object inspired by `std::error_code` for our own errors.

Things would get a little trick on the following point that we raised previously
though:

[quote]
____
[...] and we do depend on RAII to keep the code correct.
____

Imagine we have some code like the following:

[source,cpp]
----
class reference
{
public:
    reference() : L(nullptr) {}

    reference(lua_State* L)
        : L(L)
        , idx(luaL_ref(L, LUA_REGISTRYINDEX))
    {}

    ~reference()
    {
        if (!L)
            return;

        luaL_unref(L, LUA_REGISTRYINDEX, idx);
    }

    reference(reference&& o)
        : L(o.L)
        , idx(o.idx)
    {
        o.L = nullptr;
    }

    lua_State* state() const
    {
        return L;
    }

    void push() const
    {
        assert(L);
        lua_pushinteger(L, idx);
        lua_gettable(L, LUA_REGISTRYINDEX);
    }

private:
    lua_State* L;
    int idx;
};
----

If an object of this type has its destructor called on `lua_error()`-triggered
stack unwinding, it means we're manipulating the `lua_State*` (`luaL_unref(L)`
in this example) on stack unwinding (i.e. outside of a lua-catch block which
would be just after a `pcall()` return). If the VM is not in a safe state for
manipulations at this moment (this scenario just doesn't happen if you stick
with plain C which is the target lua was developed for) then we're
screwed. Luckily, the VM can handle such situations just fine as it is hinted on
the luaJIT documentation:

[quote, '<http://luajit.org/ext_c_api.html#mode_wrapcfunc>', 'Recommended usage pattern for `LUAJIT_MODE_WRAPCFUNC`']
____
[source,cpp]
----
static int wrap_exceptions(lua_State *L, lua_CFunction f)
{
  try {
    return f(L);  // Call wrapped function and return result.
  } catch (const char *s) {  // Catch and convert exceptions.
    lua_pushstring(L, s);
  } catch (std::exception& e) {
    lua_pushstring(L, e.what());
  } catch (...) {
    lua_pushliteral(L, "caught (...)");
  }
  return lua_error(L);  // Rethrow as a Lua error.
}
----
____

This guarantee is promised again (although this version of the promise is
read-only) in their “extensions” page (and again only at the _full
interoperability_ category):

[quote, '<http://luajit.org/extensions.html#exceptions> (emphasis mine)']
____
Lua errors can be caught on the C++ side with `catch(...)`. The corresponding
Lua error message *can be retrieved from the Lua stack*.
____

NOTE: Writing this project in plain C would not be a way out to at least cut
half of the problems. To do proper resource management in plain C we'd end up
resorting to endless cascades of ``pcall()``s and it wouldn't be reliable as
soon as it first hit a `LUA_ERRMEM`. {cpp} has working resource management tools
for true exceptions. It is a “take it or leave” situation here.

The final piece for our puzzle is related to async ops converting
`std::error_code` into lua exceptions (i.e. `lua_error()`). The completion
handler for async ops is not called in a lua context, so they cannot just call
`lua_error()` and hope the correct context will catch the exception (there's no
API similar to
https://www.boost.org/doc/libs/1_67_0/libs/context/doc/html/context/ff.html#context.ff.executing_function_on_top_of_a_fiber[`resume_with()`
from Boost.Context]). They need to return control to the native code that
suspended the fiber so it can throw a lua exception before control returns to
lua code.

This guarantee used to exist on luaJIT 1.x (which included Coco):

[quote, '<http://coco.luajit.org/api.html#lua_yield>']
____
Now, if the current coroutine has an associated C stack, `lua_yield()` returns
the number of arguments passed back from the resume.
____

The lack of allocated C stacks brings more complications to the implementation
that will be discussed
later. https://www.lua.org/manual/5.2/manual.html#lua_yieldk[`lua_yieldk()`]
from Lua 5.2 would be enough for us (and cheaper!),
https://github.com/LuaJIT/LuaJIT/issues/48[but we don't have that either].

Yet another option would be to set an one-time hook to be called immediately
just before resuming the lua coroutine, but it'd present challenges in the
future if we ever add debugging support, so it is avoided.

And the solution Emilua get away with is wrapping the C function inside a lua
function. The C function returns a 2-tuple. If the first argument is not nil,
the lua function itself will take care of use it to raise an error.

[source,lua]
----
local error, native = ...
return function(...)
    local e, v = native(...)
    if e then
        error(e)
    else
        return v
    end
end
----

== User-coroutines

Let's jump straight to a topic that gives some sense of continuity to the
previous section. The `pcall()` barrier is not the only barrier that the user
can insert to prevent `lua_yield()` from suspending the fiber. The user might
very well just wrap calls using `coroutine.create()`:

[source,lua]
----
function user_fn()
    coroutine.create(function()
        io_obj:emilua_async_op()
    end)
end
----

[NOTE]
--
[discrete]
== Rule

Lua's `coroutine` module must never be directly exposed to lua code.
--

The problem is solved by exposing a different `coroutine` module — a small shim
over the original one. This version inspects `this_fiber` variable for the
suspension reason (native code or lua code).

Another responsibility we throw in is check if the user is trying to operate a
fiber managed by the native scheduler (the user could get these objects through
functions such as `coroutine.running()`).

Conceptually, the implementation looks like this:

[source,lua]
----
function coroutine.resume(co, ...)
    if _G.fibers[co] ~= nil then
        error("bad coroutine", 2)
    end

    local args = table.pack(...)
    while true do
        local ret = table.pack(raw_coroutine.resume(co, table.unpack(args)))
        if ret[1] == false then
            return table.unpack(ret)
        end
        if _G.this_fiber.native_yield then
            args = table.pack(raw_coroutine.yield(table.unpack(ret, 2)))
        else
            return table.unpack(ret)
        end
    end
end

function coroutine.yield(...)
    if _G.fibers[coroutine.running()] ~= nil then
        error("bad coroutine", 2)
    end
    return raw_coroutine.yield(...)
end

coroutine.create = ...
coroutine.wrap = ...
coroutine.status = ...
coroutine.running = ...
----

== Dead fibers

When a fiber errors, the `uncaught_handler` registered at
`this_fiber.uncaught_handler` will be called. The default handler only prints a
warning on stderr or calls `os.exit(-1)` if the error originates from the main
fiber.

There isn't a `pcall` block around the whole program. `lua_resume` is enough and
it has the nice property of not unwinding the stack so it can be examined from
the error handler. A new lua thread is created to execute
`this_fiber.uncaught_handler` while it has the chance to examine the unchanged
error'ed call stack.

== Functions that receive a lua callback

There are plenty of functions that have a lua closure as a parameter
(e.g. `pcall()`, `scope()`, ...). If we blindly implement them in plain C, they
will configure a non-leaf C stack frame which we cannot suspend.

To avoid the C stack frame in the middle of the call-stack altogether, we
implement (parts of) these functions in lua, not C. The problem is then how to
expose sensitive raw resources that the C functions would use. One of the goals
is to not let these resources escape elsewhere.

// TODO (I'm not ready to invest such big effort on this early release, but
// something like this would be useful anyway by the time the project is ready
// to accommodate transpilers):
//
//Emilua will create lua closures by feeding the VM handcrafted luaJIT
//bytecode. The bytecode allows us to access upvalues by index and, in turn, set
//them from the native API with `lua_setupvalue()`. Two useful commands to examine
//luaJIT bytecode are:
//
//[source,shell]
//----
//luajit -bl test.lua
//luajit -b -t raw test.lua - | hexdump -C
//----
//
//Supporting documentation can be found at:
//
//* <http://luajit.org/running.html>
//* <http://wiki.luajit.org/Bytecode-2.0>
//* <https://github.com/LuaJIT/LuaJIT/blob/v2.0/src/lj_bc.h>
//* <https://github.com/LuaJIT/LuaJIT/blob/v2.0/src/lj_bcdump.h>

A quick way to achieve it is by having a lua bootstrap function/chunk to create
closures and later change their upvalues through C:

[source,lua]
----
local private_resource = ...
return function()
    -- use `private_resource`
end
----

This approach is naive as luaJIT 2.x does not implement some lua functions as C
functions and we cannot feed them as upvalues for the imported bytecode. For
instance, we have this behaviour for `pcall()`:

[source,cpp]
----
lua_pushcfunction(L, luaopen_base);
lua_call(L, 0, 0);
lua_getglobal(L, "pcall");
lua_CFunction pcall_addr = lua_tocfunction(L, -1);
assert(pcall_addr == nullptr); // :(
----

So a larger boilerplate is required to (1) extract an usable luaJIT bytecode and
then (2) feed it into the {cpp} source code. The solution this time also
involves the build system which will call a few luajit-powered scripts to
generate the proper bytecodes and convert them to C source using `xdd -i`.

Just to clarify: the private resource in the previous example is problematic
because `pcall()` won't be directly exposed to lua code. Therefore, the lua
bootstrap function that runs within the lua VM used by Emilua doesn't have
access to `pcall()` either. We must use another VM to create the VM
bytecode. That's where the extra build step comes in.

== Process environment

A part of the process environment (e.g. UNIX signals) should be under complete
control of the program and no external library should meddle with it. However,
no protections will be provided to enforce this good practice.

== VM settings inheritance

New actors should inherit generic customization points for the GC (e.g. step
count and period) and the JIT. They should also inherit allocator settings, but
they must *not* be prevented from creating new actors with higher allocation
quotas (unless of course the global pool is already at its limit).

== 2GB addressing limit

http://hacksoflife.blogspot.com/2012/12/integrating-luajit-with-x-plane-64-bit.html[luaJIT
has a serious 2GB limit] that has been
https://www.freelists.org/post/luajit/Fixed-a-segfault-when-unsinking-64bit-pointers[fixed
on forks]. By default, the broken 64-bit addressing mode is hidden behind
`LUAJIT_ENABLE_GC64`. Emilua might consider moving to
https://www.freelists.org/post/luajit/LuaJIT-staging-fork-to-move-the-project-forward[moonjit]
if its author don't try to part away from the lua 5.1 core and keep himself
distant from 5.3+ syntactic explosion madness. I *don't* like this {cpp}-like
culture expanding to lua or other languages (kudos to Go here for avoiding it).

== JIT parameters

The JIT parameters are also changed from the
http://luajit.org/running.html#opt_O[old defaults]:

[source,lua]
----
maxtrace=1000
maxrecord=4000
maxmcode=512  -- in KB
----

To https://github.com/openresty/luajit2#updated-jit-default-parameters[defaults
based on OpenResty findings]:

[source,lua]
----
maxtrace=8000
maxrecord=16000
maxmcode=40960  -- in KB
----

== Open questions

* Describe the behaviour for `os.exit()` when called from other VMs. Should it
  call the cancellator for every active operation? Should it exit the
  application?
* We do print to `stderr`. Shouldn't we provide an embedded logger module
  already?

== Extra caution to take when writing plug-ins

Always keep in mind:

* If you enable your IO object to be sent over channels, it'll also be able to
  migrate to a different `asio::io_context` and you must take care to keep a
  work guard to the original `asio::io_context`.

== Final note

Emilua software is complex. There should be no pursuit in indefinitely extending
this base. Rather, we should search for stabilization and maturity (and also
tooling around a solid base).

If you think there should be a nice lua library to handle IRC and what-not, by
all means do write it, but write it as a separate lua library (or native
plug-in), and compete against the free market of libraries. Do not submit a
proposal to integrate it in the core. There are no batteries included. And there
shall be no committee-driven development.

Likewise, we should be stuck in the current lua syntax (5.1 plus some extensions
found in luaJIT 2.0.5{_}footnote:[<http://luajit.org/extensions.html#lua52>
(`-DLUAJIT_ENABLE_LUA52COMPAT` will be included when I have time to build
luaJIT).]) forever. If you want more syntax, use a transpiler.
